## 音视频解码及同步处理

### 前言

查看一个视频的编码信息

![](images/av_01.png)

- 流0：视频流。

  H264：表示视频是由H264来编码

  MPEG-4：表示视频的封装格式

- 流1：音频流。

![](images/av_04.png)

### 视频的封装格式

平时我们打开电脑中自己存电影的目录的话，一般都会如下图所示，一大堆五花八门的电影。

![format](images/av_02.png)

因为下载的来源不同，这些电影文件有不同的格式，用不同的后缀表示：avi，rmvb，mp4，flv，mkv等等（当然也使用不同的图标）。在这里需要注意的是，这些格式代表的是封装格式。何为封装格式？就是把视频数据和音频数据打包成一个文件的规范。仅仅靠看文件的后缀，很难能看出具体使用了什么视音频编码标准。总的来说，不同的封装格式之间差距不大，各有优劣。

注：有些封装格式支持的视音频编码标准十分广泛，应该算比较优秀的封装格式，比如MKV；而有些封装格式支持的视音频编码标准很少，应该属于落后的封装格式，比如RMVB。

### 视频编码到播放流程

根据生活例子

![](images/av_05.png)

![](images/av_03.png)

### 视频播放器原理

音视频技术主要包含以下几点：封装技术，视频压缩编码技术以及音频压缩编码技术。如果考虑到网络传输的话，还包括流媒体协议技术。

视频播放器的源代码详细解析（Media Player Classic - HC，Mplayer，FFplay，XBMC）可以参考系列文章：

Media Player Classic：Media Player Classic - HC源代码分析 1：整体结构[系列文章]

Mplayer：MPlayer源代码分析

FFplay： FFplay源代码分析：整体流程图

XBMC： XBMC源代码分析 1：整体结构以及编译方法[系列文章]

在这里不细说了，仅简要说明一下视频播放器的原理。

视频播放器播放一个互联网上的视频文件，需要经过以下几个步骤：解协议，解封装，解码视音频，视音频同步。如果播放本地文件则不需要解协议，为以下几个步骤：解封装，解码视音频，视音频同步。

解协议的作用，就是将流媒体协议的数据，解析为标准的相应的封装格式数据。视音频在网络上传播的时候，常常采用各种流媒体协议，例如HTTP，RTMP，或是MMS等等。这些协议在传输视音频数据的同时，也会传输一些信令数据。这些信令数据包括对播放的控制（播放，暂停，停止），或者对网络状态的描述等。解协议的过程中会去除掉信令数据而只保留视音频数据。例如，采用RTMP协议传输的数据，经过解协议操作后，输出FLV格式的数据。

解封装的作用，就是将输入的封装格式的数据，分离成为音频流压缩编码数据和视频流压缩编码数据。封装格式种类很多，例如MP4，MKV，RMVB，TS，FLV，AVI等等，它的作用就是将已经压缩编码的视频数据和音频数据按照一定的格式放到一起。例如，FLV格式的数据，经过解封装操作后，输出H.264编码的视频码流和AAC编码的音频码流。

解码的作用，就是将视频/音频压缩编码数据，解码成为非压缩的视频/音频原始数据。音频的压缩编码标准包含AAC，MP3，AC-3等等，视频的压缩编码标准则包含H.264，MPEG2，VC-1等等。解码是整个系统中最重要也是最复杂的一个环节。通过解码，压缩编码的视频数据输出成为非压缩的颜色数据，例如YUV420P，RGB等等；压缩编码的音频数据输出成为非压缩的音频抽样数据，例如PCM数据。

视音频同步的作用，就是根据解封装模块处理过程中获取到的参数信息，同步解码出来的视频和音频数据，并将视频音频数据送至系统的显卡和声卡播放出来。

### 使用FFmpeg实现视频播放

写一个jni接口类，来控制底层视频的播放

```java
/**
 * 集合着jni接口和监听SurfaceView的创建过程（SurfaceHolder.Callback）
 */
public class FFmpegPlayer implements SurfaceHolder.Callback{
    static {
        System.loadLibrary("native-lib");
    }
    private SurfaceHolder surfaceHolder;
    
    /**
     * SurfaceView设置方法
     * @param surfaceView SurfaceView对象
     */
    public void setSurfaceView(SurfaceView surfaceView) {
        if (null != this.surfaceHolder) {
            this.surfaceHolder.removeCallback(this);
        }
        this.surfaceHolder = surfaceView.getHolder();
        // 设置监听
        this.surfaceHolder.addCallback(this);

    }
    public void start(final String path) {
        new Thread(new Runnable() {
            @Override
            public void run() {
                native_start(path, surfaceHolder.getSurface());
            }
        }).start();
    }
    /**
     * native函数，播放视频
     * @param path 视频本地路径
     * @param surface Surface对象
     */
    public  native void native_start(String path, Surface surface);
    @Override
    public void surfaceCreated(SurfaceHolder surfaceHolder) {
        this.surfaceHolder = surfaceHolder;
    }

    @Override
    public void surfaceChanged(SurfaceHolder surfaceHolder, int i, int i1, int i2) {   
    }

    @Override
    public void surfaceDestroyed(SurfaceHolder surfaceHolder) {
    }
}
```

由于是将播放绘制的操作交给了NDK，就需要传递播放相关的参数，路径和Surface。因为在native层是没有SurfaceView的，但是有Surface，所以需要监听SurfaceView的创建过程，通过SurfaceHolder获得Surface对象并传给native层。

在native_start函数中开始使用FFmpeg来进行视频播放。

![](images/av_06.png)

**初始化**

```c++
// 初始化FFmpeg的网络模块
avformat_network_init();

// 总上下文AVFormatContext
AVFormatContext *avFormatContext = avformat_alloc_context();

AVDictionary *param = NULL;
// 设置超时时间，单位是微秒，1秒=1000000微秒
av_dict_set(&param, "timeout", "3000000", 0);
// 打开视频文件
// 有个int返回值，0：返回成功；非0：返回失败
int ret = avformat_open_input(&avFormatContext, path, NULL, &param);
if (ret) {
    return;
}
```

> 注意：在FFmpeg中int返回值 0 都是表示成功，非0表示失败

**寻找视频流**

```c++
int video_stream_idx = -1;
// 通知FFmpeg将流解析出来
avformat_find_stream_info(avFormatContext, NULL);
// 遍历视频文件所有的流，找出视频流的索引
for (int i = 0; i < avFormatContext->nb_streams; ++i) {
    // AVMEDIA_TYPE_VIDEO：视频流
    // AVMEDIA_TYPE_AUDIO：音频流
    // AVMEDIA_TYPE_SUBTITLE：字幕流
    if (avFormatContext->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO) {
        // 如果这个流的类型是视频流，记录视频流的索引
        video_stream_idx = i;
        break;
    }
}

// 获取视频流的解码参数
// AVCodecParameters里面有对当前视频的宽度，高度，延迟时间等的描述
AVCodecParameters *codecpar = avFormatContext->streams[video_stream_idx]->codecpar;
```

**获取解码器并打开**

```c++
// 获取解码器，是与视频的id一一对应的
AVCodec *avCodec = avcodec_find_decoder(codecpar->codec_id);
// 解码器的上下文AVCodecContext
AVCodecContext *avCodecContext = avcodec_alloc_context3(avCodec);
// 将获取视频流的解码参数传入解码器上下文
avcodec_parameters_to_context(avCodecContext, codecpar);
// 打开解码器
avcodec_open2(avCodecContext, avCodec, NULL);
```
